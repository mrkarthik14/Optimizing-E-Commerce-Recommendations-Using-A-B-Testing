# A/B Testing Project: E-Commerce Recommendation Optimization

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **[üî¥ Live Streamlit Demo](https://optimizing-e-commerce-recommendations-using-a-b-testing-4yq3zq.streamlit.app/)**

> A complete, production-grade A/B testing project demonstrating rigorous statistical methodology, realistic data simulation, and executive-level reporting for optimizing e-commerce recommendations.

## üéØ Project Overview
This project simulates and analyzes an A/B test comparing **ML-powered personalized recommendations** against **rule-based recommendations** in an e-commerce setting. It demonstrates end-to-end data science workflow from experimental design through statistical analysis to business recommendations.
Project Link : https://optimizing-e-commerce-recommendations-using-a-b-testing-4yq3zq.streamlit.app/

### Business Problem

An e-commerce platform wants to replace its rule-based recommendation engine with ML-powered personalized recommendations to improve:
- **Primary KPI:** Conversion Rate (target: +5% relative lift)
- **Secondary KPIs:** Click-Through Rate, Revenue Per User
- **Guardrails:** Page load time, user engagement

### Hypothesis

ML-powered personalized recommendations will increase conversion rate by at least 5% relative lift compared to rule-based recommendations, without degrading user experience.

## üìä Key Results

| Metric | Control | Treatment | Lift | Significance |
|--------|---------|-----------|------|--------------|
| **Conversion Rate** | 15.12% | 16.40% | **+8.45%** | ‚úÖ p<0.001 |
| Click-Through Rate | 17.70% | 18.61% | +5.14% | ‚úÖ p<0.001 |
| Page Load Time | 1.216s | 1.310s | +7.70% | ‚ö†Ô∏è Degraded |

**Bayesian Analysis:** 99.99% probability that treatment is superior

**Recommendation:** ‚úÖ **LAUNCH** with performance optimization

**Estimated Impact:** $XX,XXX additional annual revenue

## üèóÔ∏è Project Structure

```
ab_testing_project/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data_generation.py      # Realistic A/B test data simulation
‚îÇ   ‚îú‚îÄ‚îÄ statistical_tests.py    # Comprehensive statistical analysis
‚îÇ   ‚îú‚îÄ‚îÄ visualization.py        # Executive-ready visualizations
‚îÇ   ‚îî‚îÄ‚îÄ app.py                  # Interactive Streamlit dashboard
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ ab_test_data.csv       # Generated dataset (237K sessions, 50K users)
‚îÇ   ‚îî‚îÄ‚îÄ analysis_results.json  # Statistical test results
‚îú‚îÄ‚îÄ figures/                    # Generated visualizations
‚îÇ   ‚îú‚îÄ‚îÄ metric_comparison.png
‚îÇ   ‚îú‚îÄ‚îÄ confidence_intervals.png
‚îÇ   ‚îú‚îÄ‚îÄ time_series.png
‚îÇ   ‚îú‚îÄ‚îÄ segment_heatmap.png
‚îÇ   ‚îú‚îÄ‚îÄ guardrail_distribution.png
‚îÇ   ‚îî‚îÄ‚îÄ cumulative_conversions.png
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ experiment_plan.md           # Experimental design document
‚îÇ   ‚îú‚îÄ‚îÄ statistical_methodology.md   # Statistical approach documentation
‚îÇ   ‚îî‚îÄ‚îÄ final_report.md              # Executive summary report
‚îú‚îÄ‚îÄ README.md                   # This file
‚îî‚îÄ‚îÄ requirements.txt            # Python dependencies
```

## üöÄ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/ab-testing-project.git
cd ab-testing-project

# Install dependencies
pip install -r requirements.txt
```

### Running the Analysis

```bash
# 1. Generate synthetic A/B test data
cd src
python data_generation.py

# 2. Run statistical analysis
python statistical_tests.py

# 3. Generate visualizations
python visualization.py

# 4. Launch interactive dashboard
streamlit run app.py
```

The dashboard will open at `http://localhost:8501`

## üìÅ Components

### 1. Data Generation (`data_generation.py`)

Simulates realistic A/B test data with:
- ‚úÖ 50,000 users over 3 weeks
- ‚úÖ User segmentation (new, casual, power users)
- ‚úÖ Temporal patterns (weekend effects, novelty decay)
- ‚úÖ Segment-specific treatment effects
- ‚úÖ Realistic noise and variance
- ‚úÖ Guardrail metric trade-offs

**Output:** `ab_test_data.csv` (237,319 sessions)

### 2. Statistical Analysis (`statistical_tests.py`)

Comprehensive testing framework:
- ‚úÖ Sample size calculation (power analysis)
- ‚úÖ Sample Ratio Mismatch (SRM) check
- ‚úÖ Covariate balance verification
- ‚úÖ Two-sample proportion tests with confidence intervals
- ‚úÖ Segmentation analysis
- ‚úÖ CUPED variance reduction
- ‚úÖ Bayesian A/B testing
- ‚úÖ Guardrail metric evaluation

**Output:** `analysis_results.json`

### 3. Visualizations (`visualization.py`)

Executive-ready charts:
- ‚úÖ Metric comparison plots
- ‚úÖ Confidence interval visualizations
- ‚úÖ Time series analysis
- ‚úÖ Segment performance heatmaps
- ‚úÖ Guardrail metric distributions
- ‚úÖ Cumulative conversion curves

**Output:** PNG files in `figures/`

### 4. Interactive Dashboard (`app.py`)

Multi-page Streamlit application:
- üìä **Executive Summary:** Go/No-Go recommendation, ROI estimate
- üìà **Statistical Analysis:** Detailed test results, SRM checks, Bayesian analysis
- üéØ **Segment Analysis:** Treatment effects by user cohort
- üöß **Guardrail Metrics:** Performance impact assessment
- üìä **Visualizations:** Interactive charts and graphs

## üìà Methodology

### Experimental Design

- **Randomization:** Stratified by user segment (50-50 split)
- **Sample Size:** 50,000 users (powered for 5% MDE at 80% power)
- **Duration:** 3 weeks (21 days)
- **Analysis Level:** User-level (to account for repeat users)

### Statistical Approach

**Frequentist:**
- Two-sample proportion test for conversion rate
- 95% confidence intervals
- Œ± = 0.05 significance threshold
- Bonferroni correction for multiple comparisons

**Bayesian:**
- Beta-Binomial conjugate priors
- 95% credible intervals
- Decision threshold: P(treatment > control) > 0.95

**Variance Reduction:**
- CUPED methodology using pre-experiment covariates
- ~XX% variance reduction achieved

### Validity Checks

‚úÖ Sample Ratio Mismatch: p=0.584 (PASSED)
‚úÖ Covariate Balance: All segments balanced
‚úÖ Temporal Stability: Effects consistent over time

## üéØ Business Impact

### Recommended Action

**‚úÖ LAUNCH with Performance Optimization**

The treatment demonstrates:
- Strong statistical evidence (p<0.001)
- Practical significance (8.45% > 5% MDE)
- High Bayesian confidence (99.99%)

However, page load time degradation requires attention.

### Revenue Impact

Assuming:
- Daily active users: ~11,000
- Annual users: ~4,000,000
- Average order value: $75

**Projected Impact:**
- Additional conversions: ~51,200/year
- Revenue increase: ~$3,840,000/year

### Rollout Strategy

1. **Phase 1 (Month 1):** Launch to Power users only (+13.8% lift, proven segment)
2. **Phase 2 (Month 2):** Expand to Casual users (+8.9% lift, large segment)
3. **Phase 3 (Month 3):** Full rollout after performance optimization
4. **Optimization:** Parallel workstream to reduce page load time by 50%

## üî¨ Key Insights

### Segment Analysis

| Segment | Lift | Significance | Interpretation |
|---------|------|--------------|----------------|
| Power   | +13.8% | ‚úÖ Significant | **Best performing** - prioritize this segment |
| Casual  | +8.9% | ‚úÖ Significant | Large volume, strong lift |
| New     | -1.0% | ‚ùå Not Significant | No effect, needs investigation |

**Insight:** Power users benefit most from personalization, likely due to richer interaction history.

### Trade-Offs

‚úÖ **Pros:**
- Significant conversion lift
- Revenue positive
- Works well for 70% of user base

‚ö†Ô∏è **Cons:**
- +7.7% page load time increase
- No benefit for new users
- Requires ML infrastructure

## üîÑ Next Steps

### Technical Optimizations

1. **Performance:**
   - Implement server-side caching (target: -50% load time)
   - Pre-compute recommendations for returning users
   - Optimize ML model inference

2. **New User Experience:**
   - Investigate why new users don't benefit
   - Develop cold-start algorithms
   - A/B test hybrid approach (rules + ML)

3. **Monitoring:**
   - Real-time performance dashboards
   - Automated alerting on metric degradation
   - Continuous learning pipeline

### Future Experiments

1. **Recommendation Display:** Test different layouts, counts, positions
2. **Personalization Depth:** Test varying levels of personalization
3. **Multi-Armed Bandits:** Dynamic allocation to best-performing variants
4. **Long-term Effects:** 6-month cohort analysis for customer lifetime value

## üìö References

### Statistical Methods
- Kohavi, R., Tang, D., & Xu, Y. (2020). *Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing*
- Deng, A., Xu, Y., Kohavi, R., & Walker, T. (2013). "Improving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data"

### Tools & Libraries
- Python 3.8+
- pandas, numpy, scipy
- matplotlib, seaborn
- streamlit (dashboard)

## üë®‚Äçüíª About This Project
https://optimizing-e-commerce-recommendations-using-a-b-testing-4yq3zq.streamlit.app/

This portfolio project demonstrates:
- ‚úÖ End-to-end A/B testing methodology
- ‚úÖ Production-quality code with documentation
- ‚úÖ Statistical rigor (frequentist + Bayesian)
- ‚úÖ Business-focused insights
- ‚úÖ Executive communication
- ‚úÖ Realistic data simulation

**Applicable to:**
- E-commerce platforms (Amazon, eBay, Shopify)
- Content recommendations (Netflix, YouTube, Spotify)
- Product features (Microsoft 365, Xbox, Azure)
- Marketing campaigns (Bing Ads, LinkedIn)

## üìß Contact

**Your Name**
- Email: charankarthiknayakanti@gmail.com
- LinkedIn: https://www.linkedin.com/in/charankarthiknayakanti/
- Portfolio: https://www.datascienceportfol.io/CHARANKARTHIKN?preview=True

## üìÑ License

MIT License - see LICENSE file for details

---

**Note:** This is a synthetic demonstration project for portfolio purposes. All data is simulated and does not represent any real company.
